data:
  train_dataset: /home/nhnghia/Projects/ViTextVQA-dataset/ViTextVQA_train.json
  val_dataset: /home/nhnghia/Projects/ViTextVQA-dataset/ViTextVQA_dev.json
  test_dataset: /home/nhnghia/Projects/ViTextVQA-dataset/ViTextVQA_test.json
  num_worker: 2

tokenizer:
  padding: max_length
  max_input_length: 80
  max_target_length: 20
  truncation: True
  return_token_type_ids: True
  return_attention_mask: True

text_embedding:
  text_encoder: VietAI/vit5-large

ocr_embedding:
  sort_type: top-left bottom-right # score or top-left bottom-right
  path_ocr: /home/nhnghia/Projects/ViTextVQA-dataset/swintext_spotter
  threshold: 0.3
  remove_accents_rate: 0
  use_word_seg: False
  max_scene_text: 32 #if not use, set 0
  d_det: 256
  d_rec: 256
  max_2d_position_embeddings: 1024
  num_distances: 32

obj_embedding:
  path_obj: /home/nhnghia/Projects/ViTextVQA-dataset/vinvl_vinvl
  use_attr: False # the  max_input_length should be set 300 if True else 100
  max_bbox: 32 #if not use, set 0
  d_obj: 2048
  d_grid: 1024

generator_args:
  max_length: 24
  min_length: 1
  num_beams: 4
  length_penalty: 1.5
  no_repeat_ngram_size: 3
  early_stopping: True

model:
  type_model: viconsformer  # Custom name for the multimodal model
  backbone: VietAI/vit5-base

train:
  output_dir: checkpoint
  cuda_device: cuda:0
  precision: float16
  with_image: True
  seed: 12345
  num_train_epochs: 100
  patience: 5
  learning_rate: 3.0e-5
  weight_decay: 1.0e-4
  metric_for_best_model: em
  batch_size: 32
